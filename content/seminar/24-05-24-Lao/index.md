---
title: "Backdoors in Deep Learning: The Good, the Bad and the Ugly"

event: CALAS Regular Meetings

location: P1402
address:
  street: 83 Tat Chee Avenue
  city: Kowloon
  region: Hong Kong
  postcode: '999077'
  country: China

summary: |
  **Speaker**: Prof. Yingjie LAO, Associate Professor, Tufts University, Boston, USA <br>
  **Time**: May 24th 2024 (Fri) at 9:00 HKT


# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: '2024-05-24T09:00:00Z'
date_end: '2024-05-24T11:00:00Z'
all_day: false

# Schedule page publish date (NOT talk date).
publishDate: '2024-05-20T11:00:00Z'

authors: []
tags: []

# Is this a featured talk? (true/false)
featured: false
reading_time: false
share: false
profile: false

url_code: ''
url_pdf: ''
url_slides: ''
url_video: ''

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides:

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects:
---
## Speaker
Prof. Yingjie LAO, Associate Professor <br> 
Tufts University, Boston, USA

## Time
May 24th 2024 (Fri) at 9:00 HKT

## Abstract
<div style="text-align: justify">
Deep learning is revolutionizing almost all AI domains and has become the core of many modern AI systems. Despite their superior performance compared to classical methods, deep learning also faces new security problems, such as adversarial and backdoor attacks, that are hard to discover and resolve due to their black-box-like property. Backdoor attacks are possible because of insecure model pretraining and outsourcing practices. Malicious third parties can add backdoors into their models or poison their released data before delivering it to the victims to gain illegal benefits. This threat seriously damages the safety and trustworthiness of AI development. While most works consider backdoors “evil”, some research explores leveraging them for positive purposes. A notable approach involves using backdoors as watermarks to detect illegal uses of commercialized data/models. Watermarks can also be used for detecting AI-generated data, particularly with the rise of large generative models like LLMs. In this presentation, I will share insights from our recent research, exploring both the “good” and “bad” aspects of backdoors in deep learning.
</div>

## Biography
<div style="text-align: justify">
Yingjie LAO is currently an associate professor in the Department of Electrical and Computer Engineering at Tufts University. He received his Ph.D. degree from the Department of Electrical and Computer Engineering at University of Minnesota, Twin Cities, in 2015. His research has been recognized with an NSF CAREER Award, an IEEE TVLSI Prize Paper Award, and an ISLPED Best Paper Award. His research interests include trusted AI, hardware security, electronic design automation, VLSI architectures for machine learning and emerging cryptographic systems, and AI for healthcare and biomedical applications.
</div>
