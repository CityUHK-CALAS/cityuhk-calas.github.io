---
title: "Jailbreak Large Language Models"

event: CALAS Regular Meetings

location: P1402
address:
  street: 83 Tat Chee Avenue
  city: Kowloon
  region: Hong Kong
  postcode: '999077'
  country: China

summary: |
  **Speaker**: Prof. Tianwei ZHANG, Assistant Professor Nanyang Technological University, Singapore<br>
  **Time**: June 25th 2024 (Tue) at 16:00 HKT


# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: '2024-06-25T16:00:00Z'
date_end: '2024-06-25T18:00:00Z'
all_day: false

# Schedule page publish date (NOT talk date).
publishDate: '2024-06-21T16:00:00Z'

authors: []
tags: []

# Is this a featured talk? (true/false)
featured: false
reading_time: false
share: false
profile: false

url_code: ''
url_pdf: ''
url_slides: ''
url_video: ''

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides:

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects:
---
## Speaker
Prof. Tianwei ZHANG, Assistant Professor <br> 
Nanyang Technological University, Singapore

## Time
June 25th 2024 (Tue) at 16:00 HKT

## Abstract
<div style="text-align: justify">
Large Language Models (LLMs) have emerged as transformative tools in the realm of artificial intelligence, powering a myriad of applications and fostering smooth human-machine interactions, particularly through chatbots like ChatGPT. However, the integration of these models introduces significant security risks. This talk focuses on one prominent security threat to LLMs, jaibreaking, which tries to deceive the models to output harmful content violating the usage policies. l will present three recent works on LLM jailbreaking. (1) A deep dive into the nature of jaibreak prompts categorizes them into unique patterns and tests their efficacy on models like GPT-3.5 and GPT-4. Our findings demonstrate the effectiveness of the jailbreak prompt and the defense power different models. (2) introducing Master key, a state-of the-art framework that not only deciphers the defensive mechanisms of popular LLM chatbots by exploiting time-based intricacies but also pioneers an automatic generation of jailbreak prompts. (3)Introducing Pandora, a comprehensive framework to jalbreak GPTs with a novel retrieval augmented generation poisoning technique. Together, these studies accentuate the pressing challenges and opportunities in securing LL-driven systems.
</div>

## Biography
<div style="text-align: justify">
Dr. Tianwei ZHANG is currently an assistant professor at College of Computing and Data Science, Nanyang Technologic University. He received his Bachelor’s degree at Peking University in 2011. and Ph.D degree at Princeton University in 2017. His research focuses on building efficient and trustworthy computer systems. He has been involved in the organization committee numerous technical conferences, including serving as the general chair of KSEM’22. He serves on the editorial board of lEEE Transactions on Circuits and Systems for Video Technology (TCSVT) since 2021, and receives the best associate editor award in2023. He has published more than 130 papers in top-tier Al, system and security conferences and journals. He has received several best paper awards including ASPLOS’23, ICDlS’22 and ISPA’21.

</div>
